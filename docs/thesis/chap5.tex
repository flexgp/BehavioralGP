\chapter{Conclusion}
\label{chap:conclusion}

\section{Contributions} 
\label{section:contributions}
My primary contributions are threefold.

\begin{enumerate}[noitemsep]
\item Provide support for the claims of BGP by Krawiec et al.
\item Create a BGP implementation that is easily extendable for future work related to BGP.
\item Explore numerous extensions to and features of the BGP methodology.
\end{enumerate}

\section{Future Work}
\label{section:future_work}
A lot of what this work brings to light is particular paths to extend the concepts and understanding of BGP.  Below I detail several avenues to explore.

\subsubsection{Alternate Models}
The primary avenue that this work opens up is the possibility of exploring different models to use in BGP.  It would be interesting to see if using a machine learning model whose purpose is more inline with what BGP asks for would benefit the evolutionary process.  For example, instead of building an entire machine learning model on the trace, one could use a feature selection technique, or measure the statistical correlation between the columns.  The output would provide material with which to populate the archive.  However, this would not provide additional fitness measures.

\subsubsection{Lasso}
Lasso brought several interesting features of BGP to light.  It is important to have a machine learning model that is robust against the pathological inputs that can be generated by a genetic programming algorithm.  It would be interesting to explore why Lasso in general, or at least the implementation that I use has significant trouble for certain inputs.  Additionally, it is interesting that even though running Lasso is the runtime bottle neck, the configurations that took substantially too long to run were BP2A, and BP4A, both of which use an archive.  Understanding precisely why the configurations with an archive produce worse inputs to Lasso could be insightful.

\subsubsection{Mixing Traces}
It is unclear exactly why the BGP model that uses the combined traces of all of the programs in the population performed less well than running the model on each program trace independently.  It is possible that the idea has merit, but the particulars were not a good fit for BGP.  In particular, in each generation only a single machine learning model is built.  Therefore, all of the selected trees put into the archive in a single generation have the same weight.

An alternate implementation would be to draw random subsets from the combined trace of the programs in the population, and build a model on each.  This would create many candidate subtrees with different weights, and possibly a more robust archive, if it can be populated with subtrees that are frequently selected by the machine learning model.

\subsubsection{Subtrees with the Same Semantics}
As is mentioned in Section \ref{section:evaluation} if two subtrees have identical columns in the program trace (i.e. identical \textit{semantics}), only the smaller subtree is kept.  This introduces a bias that is not necessarily beneficial to the evolutionary process.  It would be interesting to explore how common subtrees with identical semantics are, and if choosing the smaller tree is the better choice.

\subsubsection{Combining Reproduction Operators}
It would be interesting to see if combining mutation, crossover, and archive-based crossover could enable better performance than using only two of the reproduction operators.

%% This subsubsection was the result of a bug in the implementation for FlexGP.  It should have been the sum of the fitness measure differents, not the product.  
%
%\subsubsection{Program Size and Model Complexity Fitness Functions}
%I find that for both the program size fitness function, and the model complexity fitness function, a lot of programs in the population have the same values as other programs in the population.  This frequently results in these programs having a crowding distance of zero.  It would be interesting to try using a fitness measure that has more diversity between programs in the population.  Alternatively, one could try out setting the minimum possible distance between two programs for a each fitness function as some $\epsilon$.  This would allow programs that have identical values for one fitness function to be compared relative to each other based on the diversity of the output of the other fitness functions.

%\subsubsection{Model Error}
%Comparing the performance of REPTree vs the Scikit Learn decision tree, I find that REPTree typically builds models with much higher errors when run on the trace, while Scikit Learn builds models with near zero error.  It would be interesting to explore what impact this has to the evolutionary process.

\subsubsection{Model Evaluation}
In BGP, after the model is built on the trace of each program, it is evaluated on the trace.  The result of its evaluation is used as the output of one of the fitness functions for the program.  It would be interesting to explore evaluating each model on a test set, instead of the trace.  Perhaps this would yield a more useful fitness function.

%\section{Conclusion}
