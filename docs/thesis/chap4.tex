\chapter{Experiments}
\label{chap:experiments}

\section{Setup}
\label{section:setup}
In order to test the performance of different behavioral genetic programming models, I run 16 distinct configurations of BGP on the same 17 data sets that are used for the task of symbolic regression by Krawiec et al.  The basis of the data sets is taken from a paper entitled \textit{Genetic Programming Needs Better Benchmarks} by McDermott et al. \cite{benchmarks} The complete specification of all 17 data sets is presented in Appendix \ref{appendix:data_sets}.

The basis of the 16 BGP configurations that I use, comes from the 3 BGP configurations used by Krawiec et al.:

\begin{enumerate}[noitemsep]
\item \textbf{BP2A} uses only the program error fitness function and the program size fitness function (Equations \ref{eq:program_error} and \ref{eq:program_size}), but replaces crossover with archive-based crossover.
\item \textbf{BP4} uses all four BGP fitness functions (Equations \ref{eq:program_error}, \ref{eq:program_size}, \ref{eq:model_complexity}, and \ref{eq:model_error}), but performs ordinary crossover.
\item \textbf{BP4A} uses all four BGP fitness functions, and replaces crossover with archive-based crossover.
\end{enumerate}

%I use a population size of 100 programs, a mutation rate of $0.1$, and a crossover rate of $0.9$.  The mutation rate and crossover rate are the relative probability that each operation is used.  Each run terminates after 250 generations.

Note that in all three variants, the same model is constructed.  The only distinction is for what the model is used.  In addition to running the three configurations above using the REPTree model (as is done by Krawiec et al.), I run each of the three configurations using each of the models specified in Section \ref{section:extensions}.  I also explore several other parameter configurations. I explore the use of a larger archive, and the use of different mutation and archive-based crossover rates.  Specifically, in several runs I use a mutation rate of $0.05$, and a archive-based crossover rate of $0.95$.  The reason for this last variant is that GP crossover creates two new programs each time it is called.  However, archive-based crossover only creates one new program.  These altered operation rates closely simulate the creation of two new programs each time archive-based crossover is called.

For a baseline comparison, I run conventional GP with the two conventional GP fitness functions described in Section \ref{section:evaluation}.  For all of the runs, I use the same parameters that are specified by Krawiec et al.  A complete list of the parameters that are fixed for every run can be found in Appendix \ref{appendix:fixed_parameters}.  The following is a complete list of the 17 configurations (16 BGP, 1 conventional GP) that I run.  Their exact specifications are detailed in Appendix \ref{appendix:configuration_parameters}.

\begin{multicols}{2}
\begin{enumerate}
\item GP
\item BP2A - REPTree
\item BP2A - Full Pop
\item BP2A - Lasso
\item BP2A - Scikit Learn
\item BP2A - Randomized
\item BP2A - Larger Archive
\item BP2A - Different Rates
\item BP4 - REPTree
\item BP4 - Lasso
\item BP4 - Scikit Learn
%\item BP4 - Randomized
\item BP4A - REPTree
\item BP4A - Lasso
\item BP4A - Scikit Learn
\item BP4A - Randomized
\item BP4A - Larger Archive
\item BP4A - Different Rates
\end{enumerate}
\end{multicols}

For the majority of the configurations, I perform 30 runs on each data set.  However, all of the Scikit Learn configurations take much longer to run because they require calling Python from within Java.  Therefore, they are run 5 times each.  Additionally, BP2A - Lasso, and BP4A - Lasso, both did not run to completion.  As a result, they are omitted from the discussion.  They are addressed in Section \ref{section:future_work}.

\section{Results}
Appendix \ref{appendix:results} contains tables for the average and standard deviation of program fitness, program size, and program runtime, for the best of run programs for each configuration and data set.  A best of run program is the program with the lowest program error generated in a given run.  Additionally, there is a table with the percentage of runs that produced a perfect individual for each configuration and dataset.  Below I discuss the results in aggregate and compare them to those of Krawiec et al.

%\input{avg_fitness}

Table \ref{table:ranks} contains the average rank for each configuration across all data sets for a variety of metrics.

%, and Table \ref{table:avg_fitness} contains the average program error for the best of run program for each configuration and data set.

\subsubsection{Average Rank for BGP with REPTree}
Krawiec et al found that the three BGP configurations that they run rank as follows for the given metrics:

\begin{itemize}[noitemsep]
\item \textbf{Program fitness}: BP4A, BP2A, BP4, GP
\item \textbf{Number of perfect programs found}: BP4A, BP2A, BP4, GP
\item \textbf{Program size}: GP, BP4, BP2A, BP4A
\item \textbf{Program runtime}: GP, BP4, BP4A, BP2A
\end{itemize}

It is important to note that the above ranks by Krawiec et al. are calculated based on the performance of each configuration across multiple task domains: boolean, categorical, and regression, while I only implement BGP for regression.  Both Krawiec et al., and I find that program fitness rank is inversely proportional to program size rank.  For program runtime, both sets of results suggest that GP is by far the fastest, and using an archive is slower than not.  For program fitness, both Krawiec et al., and I find that using an archive results in a better fitness than if an archive is not used, however, my results suggest that the added fitness functions hurt the evolutionary process.  Considering average fitness rank, it seems as though the BGP paradigm of replacing crossover with archive-based crossover is certainly beneficial.  However, considering the number of perfect programs found, my results suggest that configurations with an archive do not perform as well.

\begin{table*}[ht]
\centering
\begin{adjustbox}{width=1\textwidth}
\small
\begin{tabular}{ c | c c | c c }
\hline\hline
& Average Fitness Rank & & Average Rank For Finding Perfect Programs & \\
\hline
1 & BP2A - Different Rates & 3.53 & GP & 1.47 \\
2 & BP2A - REPTree & 4.06 & BP2A - Scikit Learn & 2.06 \\
3 & BP2A - Scikit Learn & 4.71 & BP4 - Scikit Learn & 2.06 \\
4 & GP & 5.35 & BP2A - Larger Archive & 2.59 \\
5 & BP2A - Larger Archive & 5.41 & BP2A - REPTree & 2.65 \\
6 & BP4A - Scikit Learn & 5.65 & BP4 - Lasso & 2.76 \\
7 & BP4 - Scikit Learn & 6.06 & BP2A - Randomized & 2.82 \\
8 & BP2A - Randomized & 6.18 & BP2A - Different Rates & 2.88 \\
9 & BP2A - Full Pop & 9.94 & BP4 - REPTree & 2.88 \\
10 & BP4 - Lasso & 10.88 & BP4A - Scikit Learn & 2.88 \\
11 & BP4A - Different Rates & 11.12 & BP4A - Larger Archive & 3.59 \\
12 & BP4A - REPTree & 11.53 & BP4A - Randomized & 3.65 \\
13 & BP4A - Larger Archive & 11.65 & BP4A - REPTree & 3.88 \\
14 & BP4 - REPTree & 11.88 & BP4A - Different Rates & 4.18 \\
15 & BP4A - Randomized & 12.06 & BP2A - Full Pop & 4.76 \\
\hline\hline
& Average Size Rank & & Average Runtime Rank & \\
 \hline
1 & BP4 - REPTree & 3.0 & GP & 1.0 \\
2 & BP4 - Lasso & 3.88 & BP4 - REPTree & 2.0 \\
3 & BP4 - Scikit Learn & 5.82 & BP2A - Different Rates & 3.24 \\
4 & BP4A - Randomized & 5.94 & BP4A - Larger Archive & 5.41 \\
5 & BP4A - REPTree & 6.35 & BP4A - Different Rates & 5.41 \\
6 & GP & 6.65 & BP2A - Larger Archive & 5.94 \\
7 & BP4A - Larger Archive & 7.06 & BP2A - Randomized & 6.47 \\
8 & BP4A - Different Rates & 7.29 & BP4A - Randomized & 7.76 \\
9 & BP2A - Scikit Learn & 9.41 & BP2A - REPTree & 8.82 \\
10 & BP2A - Different Rates & 9.41 & BP4A - REPTree & 9.18 \\
11 & BP4A - Scikit Learn & 9.53 & BP2A - Full Pop & 10.76 \\
12 & BP2A - Randomized & 10.0 & BP4 - Lasso & 12.18 \\
13 & BP2A - Larger Archive & 10.24 & BP2A - Scikit Learn & 13.29 \\
14 & BP2A - REPTree & 10.35 & BP4 - Scikit Learn & 13.94 \\
15 & BP2A - Full Pop & 14.88 & BP4A - Scikit Learn & 14.59 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Average rank of each configuration across all data sets.}
\label{table:ranks}
\end{table*}

%\begin{table*}[ht]
%\centering
%\begin{adjustbox}{width=1\textwidth}
%\small
%\begin{tabular}{ c | c c | c c }
%\hline\hline
%& Average Fitness Rank & & Average Rank For Finding Perfect Programs & \\
%\hline
%1 & BP2A - Different Rates & 3.53 & GP & 1.53 \\
%2 & BP2A - REPTree & 4.12 & BP2A - Scikit Learn & 2.12 \\
%3 & BP2A - Scikit Learn & 4.82 & BP4 - Scikit Learn & 2.18 \\
%4 & GP & 5.35 & BP2A - Larger Archive & 2.71 \\
%5 & BP2A - Larger Archive & 5.53 & BP2A - REPTree & 2.82 \\
%6 & BP4A - Scikit Learn & 5.65 & BP4 - Randomized & 2.88 \\
%7 & BP4 - Scikit Learn & 6.12 & BP2A - Randomized & 3.0 \\
%8 & BP2A - Randomized & 6.24 & BP2A - Different Rates & 3.0 \\
%9 & BP2A - Full Pop & 10.35 & BP4 - Lasso & 3.0 \\
%10 & BP4 - Lasso & 11.18 & BP4A - Scikit Learn & 3.0 \\
%11 & BP4A - Different Rates & 11.47 & BP4 - REPTree & 3.06 \\
%12 & BP4A - REPTree & 11.94 & BP4A - Larger Archive & 3.76 \\
%13 & BP4A - Larger Archive & 11.94 & BP4A - Randomized & 3.88 \\
%14 & BP4 - REPTree & 12.41 & BP4A - REPTree & 4.06 \\
%15 & BP4A - Randomized & 12.53 & BP4A - Different Rates & 4.47 \\
%16 & BP4 - Randomized & 12.82 & BP2A - Full Pop & 5.06 \\
%\hline\hline
%& Average Size Rank & & Average Runtime Rank & \\
% \hline
%1 & BP4 - Randomized & 3.0 & GP & 1.0 \\
%2 & BP4 - REPTree & 3.41 & BP4 - REPTree & 2.0 \\
%3 & BP4 - Lasso & 4.41 & BP4 - Randomized & 3.65 \\
%4 & BP4 - Scikit Learn & 6.53 & BP2A - Different Rates & 3.88 \\
%5 & BP4A - Randomized & 6.88 & BP4A - Larger Archive & 6.41 \\
%6 & BP4A - REPTree & 7.29 & BP4A - Different Rates & 6.41 \\
%7 & GP & 7.47 & BP2A - Larger Archive & 6.82 \\
%8 & BP4A - Larger Archive & 8.0 & BP2A - Randomized & 7.35 \\
%9 & BP4A - Different Rates & 8.29 & BP4A - Randomized & 8.76 \\
%10 & BP2A - Scikit Learn & 10.24 & BP2A - REPTree & 9.76 \\
%11 & BP2A - Different Rates & 10.41 & BP4A - REPTree & 10.18 \\
%12 & BP4A - Scikit Learn & 10.41 & BP2A - Full Pop & 11.76 \\
%13 & BP2A - Randomized & 11.0 & BP4 - Lasso & 13.18 \\
%14 & BP2A - Larger Archive & 11.24 & BP2A - Scikit Learn & 14.29 \\
%15 & BP2A - REPTree & 11.35 & BP4 - Scikit Learn & 14.94 \\
%16 & BP2A - Full Pop & 15.88 & BP4A - Scikit Learn & 15.59 \\
%\hline
%\end{tabular}
%\end{adjustbox}
%\caption{Average rank of each configuration across all data sets.}
%\label{table:ranks}
%\end{table*}

%\begin{table*}[ht]
%\centering
%\begin{adjustbox}{width=1\textwidth}
%\small
%\begin{tabular}{ c c c }
%\hline\hline
%& Average Fitness Rank & Average Rank For Finding Perfect Programs \\
%\hline
%1 & BP4A & BP4A \\
%2 & BP2A & BP2A \\
%3 & BP4 & BP4 \\
%4 & GP & GP \\
%\hline\hline
%& Average Size Rank & Average Runtime Rank \\
% \hline
%1 & GP & GP \\
%2 & BP4 & BP4 \\
%3 & BP2A & BP4A \\
%4 & BP4A & BP2A \\
%\hline
%\end{tabular}
%\end{adjustbox}
%\caption{Average rank of each configuration across all data sets.}
%\label{table:ranks}
%\end{table*}

\subsubsection{Full Population Model}
It seems that running the model on the each program trace is almost always better than running the model on the combined trace of the entire population.  In the full population case, each generation, the archive is only populated with subtrees taken from a single model, which might result in a less diverse archive.  Further work is needed to understand precisely why this model performs less well.

\subsubsection{Lasso Model}
The most significant feature of BGP that this configuration brings to light is that it is important to use a highly robust machine learning model for BGP.  Even for BP4 - Lasso, which did run to completion, the standard deviation of the runtimes are by far the largest (see Table \ref{table:std_time}).  The results also suggest, that for BP4, Lasso may provide better additional fitness measures than REPTree.

\subsubsection{Scikit Learn Model}
Because each BGP configuration using the Scikit Learn Model was only run five times on each data set, it is hard to make strong conclusions about its performance.  However, it seems that the resulting fitness is neither conclusively better nor worse than using REPTree.  However, the runtimes of all of the Scikit Learn Model configurations were notably the longest.  This is primarily due to the overhead of running a model that is written in the Python programming language, and calling it from Java.

\subsubsection{Randomized Model}
The randomized model consistently performed worse than the model that used the trees generated by REPTree.  This gives confidence to the claim that the subtrees generated by REPTree are more beneficial for driving the evolutionary process. 

\subsubsection{Larger Archive}
The results suggest that using a larger archive does not substantially help nor harm the resulting fitness of the generated programs.

\subsubsection{Different Rates}
It does appear that for both relevant BGP configurations, using a higher archive-based crossover rate, slightly improves the resulting program fitness for a given run.  This seems to invalidate the possibility that BGP only performed better than GP because the effective reproduction operator probabilities were substantially different from conventional GP.  It is also noteworthy, that using a higher archive-based crossover rate decreases the average runtime.


