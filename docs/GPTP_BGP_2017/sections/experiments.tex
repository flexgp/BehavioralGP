
\section{Experiments}\label{sect:experiments}

We start this section by detailing the benchmarks we use, the parameters of our algorithms and name our algorithm configurations for convenience. Section~\ref{sect:ftr-select} then evaluates the impact of different decision tree algorithms. Section~\ref{sect:agg-features} evaluates the performance of \FULL and \DRAW for different configurations then compares \FULL, \DRAW and program-based model techniques.
 
\subsection{Experimental Data, Parameters}\label{sect:data_sets}

Our investigation uses 17 symbolic regression benchmarks from~\cite{benchmarks}. All of the benchmarks are defined such that the dependent variable is the output of a particular mathematical function for a given set of inputs.  All of the inputs are taken to form a grid on some interval.  Let $E[a, b, c]$ denote $c$ samples equally spaced in the interval $[a,b]$. (Note that McDermott et al. defines $E[a, b, c]$ slightly differently.)  Below is a list of all of the benchmarks that are used:

\begin{enumerate}
\item \textbf{Keijzer1}: $0.3x \sin(2 \pi x);$ $x \in E[-1,1,20]$
\item \textbf{Keijzer11}: $x y+\sin((x-1)(y-1));$ $x, y \in E[-3,3,5]$
\item \textbf{Keijzer12}: $x^{4}-x^{3}+\frac{y^{2}}{2}-y;$ $x, y \in E[-3,3,5]$
\item \textbf{Keijzer13}: $6 \sin(x) \cos(y);$ $x, y \in E[-3,3,5]$
\item \textbf{Keijzer14}: $\frac{8}{2 + x^{2} + y^{2}};$ $x,y \in E[-3,3,5]$
\item \textbf{Keijzer15}: $\frac{x^{3}}{5} - \frac{y^{3}}{2} - y - x;$ $x, y \in E[-3,3,5]$
\item \textbf{Keijzer4}: $x^{3} e^{-x} \cos(x) \sin(x) (\sin^{2}(x) \cos(x) - 1);$ $x \in E[0,10,20]$
\item \textbf{Keijzer5}: $\frac{3 x z}{(x - 10) y^{2}};$ $x,y \in E[-1,1,4]; z \in E[1,2,4]$
\item \textbf{Nguyen10}: $2 \sin(x) \cos(y);$ $x,y \in E[0,1,5]$
\item \textbf{Nguyen12}: $x^{4} - x^{3} + \frac{y^{2}}{2} - y;$ $x,y \in E[0,1,5]$
\item \textbf{Nguyen3}: $x^{5} + x^{4} + x^{3} + x^{2} + x;$ $x \in E[-1,1,20]$
\item \textbf{Nguyen4}: $x^{6} + x^{5} + x^{4} + x^{3} + x^{2} + x;$ $x \in E[-1,1,20]$
\item \textbf{Nguyen5}: $\sin(x^{2}) \cos(x) - 1;$ $x \in E[-1,1,20]$
\item \textbf{Nguyen6}: $\sin(x) + \sin(x + x^{2});$ $x \in E[-1,1,20]$
\item \textbf{Nguyen7}: $\ln(x + 1) + \ln(x^{2} + 1);$ $x \in E[0,2,20]$
\item \textbf{Nguyen9}: $\sin(x) + \sin(y^{2});$ $x,y \in E[0,1,5]$
\item \textbf{Sext}: $x^{6} - 2 x^{4} + x^{2};$ $x \in E[-1,1,20]$
\end{enumerate}

We use a standard implementation of GP and chose parameters according to settings documented in~\cite{krawiecGecco2014}.


\textbf{Fixed Parameters}\label{appendix:fixed_parameters}
\begin{itemize}
\item \textbf{Tournament size}: 4
\item \textbf{Population size}: 100
\item \textbf{Number of Generations}: 250
\item \textbf{Maximum Program Tree Depth}: 17
\item \textbf{Function set}\footnote{Note that for our implementation of $/$, if the denominator is less than $10^{-6}$ we return $1$, and for our implementation of $\log$, if the denominator is less than $10^{-6}$ we return $0$.}: $\{ +, -, *, /, \log, \exp, \sin, \cos, -x \}$
\item \textbf{Terminal set}: Only the features in the benchmark.
\item \textbf{Archive Capacity}: 50
\item \textbf{Mutation Rate $\mu$}: 0.1
\item \textbf{Crossover Rate with Archive configuration $\chi$}: 0.0
\item \textbf{Crossover Rate with GP $\chi$}: 0.9
\item \textbf{Archive-Based Crossover Rate $\alpha$}: 0.9
\item \textbf{REPTree}  defaults but no pruning
\item \textbf{\SCIKIT} defaults
\item \textbf{Number of runs} 30
\end{itemize}

First we use the 3 BGP algorithm configurations that use \REPTREE to replicate \cite{krawiecGecco2014}'s work on the symbolic regression benchmarks. These we call BGP2A, BGP4, BGP4A following precedent. In the name the digit 2 indicates that model error $e$ and complexity $c$ were not integrated into program fitness while 4 indicates they were.   The suffix A indicates whether or not subprograms from the model were qualified for archive insertion and archive retrieval during BGP crossover. When the A is omitted ordinary crossover is used. We observe results consistent with the prior work. Our open source software is available on Github.
This allowed us to proceed to evaluate feature selection sensitivity based on modeling algorithm.

It is important to note, that for each configuration we report regression, i.e. training set performance. We are primarily interested in exploring subprogram behavior and how to assemble subprograms. Reporting generalization would complicate the discussion without materially affecting our conclusions. 

\subsection{Sensitivity to Model Bias}\label{sect:ftr-select}

Q1. Does the feature selection bias of the model step matter? 

\input{sections/decision_tree_ranks}

Table~\ref{table:ranksReTreeVCART} shows the results of running the 3 different configurations (BGP2A, BGP4, BGP4A) each with the two decision tree algorithms.  Averaging over the rankings across each benchmark we find that BP2A using \REPTREE is best. For BPG2A, \REPTREE outranks \SCIKIT but when model error is integrated into the program fitness, (i.e. BPG4A and BPG4) regardless of whether or not an archive is used, \SCIKIT is superior to \REPTREE.  

When we compare the results of using the archive while model error is integrated into the program fitness (i.e.  BPG4A to BPG4), for both \REPTREE and \SCIKIT it is better to use an archive than to forgo one.  Comparing BPG2A with BPG4A, we can measure the impact of model error and complexity integration.  We find that for both \SCIKIT and \REPTREE it is not helpful to integrate model error and complexity into program fitness. 

For a deeper dive, at the specific benchmark level, Table~\ref{table:fitnessReTreeVCART} shows the average best fitness at end of run (of 30 runs), for each benchmark.   Averaging all fitness results, no clear winner is  discernible. For certain comparisons \SCIKIT will be superior while for others \REPTREE is.   We also show one randomly selected run of Keijzer1 running with \REPTREE modeling and configuration BPG4 in Figure~\ref{fig:deepdive}.   We plot on the first row  model error on the left and the fitness of the best program (right).  The plots on the second row show number of features of model  and number of subprograms in the best program (right). The plots on the third row show the ratio of number of model features to program subtrees (left) and ratio of model error to program fitness. Since the run is configured for  BPG4 program fitness integrates both model error and complexity.  No discernible difference arose among this sort of plot. This is understandable given the stochastic nature of BGP.

\input{sections/decision_tree_fitness}

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.99\linewidth]{sections/figures/figure_reptree.png}
\caption{We take one run of Keijzer1 running with \REPTREE modeling and configuration BP4.   We plot on the first row  model error on the left and the fitness of the best program (right).  The plots on the second row show number of features of model  and number of subprograms in the best program (right). The plots on the third row show the ratio of number of model features to program subtrees (left) and ratio of model error to program fitness. Since the run is configured for  BP4 program fitness integrates both model error and complexity.}
\label{fig:deepdive}
\end{center}
\end{figure}

%
%Describe difference in implementations between Reptree and SKL-RepTree.\\
%Need REPTREE and Scikit learn algorithm references and links to their code.
%
%Compare REPTREE TO SICKIT LEARN for BGP 2A, 4a, 4 getting 6 combinations\\
%Reference Table~\ref{table:avg_fitness} comparing average program error among BGP2A, BGP4, BGP4A, GP for 17 functions with REPTREE and ScikitLearn implementation.\\
%Reference Table~\ref{table:avg_size} showing average program size among BGP2A, BGP4, BGP4A, GP for 17 functions with REPTREE and ScikitLearn implementation.\\
%STDEV is in separate table in thesis, how to handle in paper?
%
%Statistical testing required!!
%
%Include a ranking table.  just on error of program and only 6 combinations (two choices of decision tree implementation) and 3 algorithms (2A,4,4A). 
%
%Select 1 run, 1 dataset: what is \# of features in model of best individual at first and final generation (each generation)? What is fraction of \# of features in model to \# of subtrees in best individual at first and final generation (each generation)?  (Each generation) means we would have a plot (fitness on Y1 axis, features/fraction on Y2, Y3 and X is generations). We could amass these statistics for every dataset and 30 runs but lower priority than other tasks.
%
We conclude that in this case of different decision tree algorithms perhaps the subtlety of contrast is not strong enough.  

\subsection{Aggregate Trace Matrices}\label{sect:agg-features}
In this section, we compare various configurations of \FULL and \DRAW.  For the algorithm configurations of this section, we adopt a clearer notation. We drop the BGP prefix and use $M$ to denote when program contribution is integrated into program fitness, and $\hat M$ to denote when it is not. We use $A$ to denote when subprograms are qualified for archive insertion and archive retrieval during BGP crossover, and $\hat A$ to denote when ordinary crossover is used.

More details of the \DRAW method are appropriate.  Referencing \cite{krawiecGecco2014} we analyze the formula for computing the weight of a given subtree (see Equation~\ref{eq:subtree_weight}).  We note that the $|U(p)|$ factor in its denominator indirectly increases the weight of smaller subprograms.  This occurs because smaller programs yield smaller models (i.e. smaller $|U(p)|$), and smaller programs have smaller subprograms.  Therefore we designed \DRAW to also favor the archiving of smaller subprograms.  \DRAW proceeds as follows:

\begin{inparaenum}
\item The population is sorted best to worst by program fitness (program error and size) using the NSGA pareto front crowding calculation because BGP is multi-objective.

\item The sorted population is cut off from below at a threshold $\lambda \%$ to form $C$.  The trace matrixes of every program in $C$ are concatenated to form $T_C$ which we call the subprogram pool.  

\item We next sort the population by \textbf{size} and select the smallest $20\%$ forming a size sample we call $K$.
   
\item Finally we draw from $K$ at random to obtain the number of subprograms that will be collectively modeled. Then we select the equivalent number of columns at random from $T_C$ and form a model. We repeat this step each time for the size of the population.  This generates multiple smaller collections of diverse subprograms. 

\end{inparaenum}

\noindent Q2. Can trace matrix concatenation which pools subprograms among different programs improve BGP performance?

\input{sections/draws_average_ranks}
%\input{sections/fullpop_average_ranks}

\input{sections/average_fitness_DF}

\input{sections/ranks_all}

We first asked what if  $C$ is composed of \textbf{every} subprogram in the population, i.e. $|C| = PopSize$?  While this $C$ using \FULL would only support one model being derived, it would give all subprograms in the population an opportunity to be used with each other in the model as features.  Similarly, by favoring many smaller combinations drawn from all subprograms, \DRAW would, through repetition,  give all subprograms in the population an opportunity to be used with some of the all the others.   If we compare the result of \DRAW and \FULL we can gauge the difference between generating many more small models vs one bigger model, when every subprogram in the population is ``eligible'' to be selected as a model feature.   This comparison is detailed on the bottom line of Table \ref{table:XXdraws_avg_ranks}. The leftmost averaged ranking results (by average fitness, across the 17 benchmarks) for different model and archive options are from \DRAW and the rightmost are from \FULL. The data reveals that using all the subprograms, with either \FULL or \DRAW is NOT advantageous. Further empirical investigation to understand this result should consider two issues: \begin{inparaenum} \item the program size to fitness distribution of the population each generation could be leading to very large number of subprograms and \item the modeling algorithm (\REPTREE) may be overwhelmed, in the case of \FULL, by the number of features, given the much smaller number of training cases for the regression. \end{inparaenum}

Next we can consider the rankings of each configuration across different selections for the subprogram pool $C$.  When $\lambda=25$ the model feature options are from the highest fitness tier of the population. In 4 of 6 cases, this appears to \textit{impede} the error of the best of run program, as measured by average ranking.  In 4 of 6 cases, including all 3 of \DRAW,  sizing the subprogram pool to be slightly less elitist ($\lambda=50$ or $\lambda=75$) was better. But extending $\lambda$ to 100 appears to be too diverse.  Table~\ref{table:fitness_draw_full} and Table~\ref{table:rank_program_error_draw_full} provide more detailed average fitness and ranking information, i.e. results for each individual benchmark.  

Finally, we compare these configurations to the three original BGP configurations.  We find that the best performing method is highly dependent on the specific benchmark, and that overall none of the configurations is shown to be the clear winner.

%show discuss results of full-pop (call it 100\_XX)\\
%
%Detailed study: take 1 run, 1 dataset (save seed!) and one algorithm of 2A,3,3A and one choice of decision tree.  What dataset? Choices: use hardest problem, or one that 2A,3,3A don't do as well as GP on. \\
%How many programs have one or more features in the model?\\ how many programs have 2, 3, etc? This says something about co-adaptation. Were pairs of features subtrees where one was within the other?  \\what is the ratio of model features to total-features-in-pop? \\how many features are in the model (first, final generations?) (maybe plot every generation ( very low priority))
%
%Now what if we discriminate the aggregation set by fitness so we're only identifying features from superior parts of the population?
%
%We create 3 additional  aggregate trace matrices: 25, 50, 75,  where pop is sorted low to high by fitness (error + size) and then cut off at these different thresholds. 
%
%Show/describe a longitudinal comparison of all 4 and add in best of program-trace resutls.
%
%Does the discrimination by program fitness plus the aggregation among programs work better than just program trace? SHOW GRAND RANKING.

%\TODO{Insert my sketch of tables of results}
%
%\TODO{Not for Monday night but if we get accepted, we can increase the number of runs and robustness of the detailed results to encompass more datasets or more runs and provide average and standard deviation}
