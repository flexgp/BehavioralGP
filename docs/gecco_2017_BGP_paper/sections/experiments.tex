
\section{Experiments}\label{sect:experiments}

We start this section by detailing the benchmarks we use, the parameters of our algorithms and name our algorithm configurations for convenience. Section~\ref{sect:ftr-select} then evaluates the impact of different decision tree algorithms. Section~\ref{evaluates} the performance of \FULL and \DRAW for different configurations then compares the \FULL, \DRAW and program-based model techniques.
 
\subsection{Experimental Data, Parameters}\label{sect:data_sets}

Our investigation uses 17 symbolic regression benchmarks from~\cite{benchmarks}. All of the benchmarks are defined such that the dependent variable is the output of a particular mathematical function for a given set of inputs.  All of the inputs are taken to form a grid on some interval.  Let $E[a, b, c]$ denote $c$ samples equally spaced in the interval $[a,b]$. (Note that McDermott et al. defines $E[a, b, c]$ slightly differently.)  Below is a list of all of the benchmarks that are used:

\begin{enumerate}[noitemsep]
\item \textbf{Keijzer1}: $0.3x \sin(2 \pi x);$ $x \in E[-1,1,20]$
\item \textbf{Keijzer11}: $x y+\sin((x-1)(y-1));$ $x, y \in E[-3,3,5]$
\item \textbf{Keijzer12}: $x^{4}-x^{3}+\frac{y^{2}}{2}-y;$ $x, y \in E[-3,3,5]$
\item \textbf{Keijzer13}: $6 \sin(x) \cos(y);$ $x, y \in E[-3,3,5]$
\item \textbf{Keijzer14}: $\frac{8}{2 + x^{2} + y^{2}};$ $x,y \in E[-3,3,5]$
\item \textbf{Keijzer15}: $\frac{x^{3}}{5} - \frac{y^{3}}{2} - y - x;$ $x, y \in E[-3,3,5]$
\item \textbf{Keijzer4}: $x^{3} e^{-x} \cos(x) \sin(x) (\sin^{2}(x) \cos(x) - 1);$ $x \in E[0,10,20]$
\item \textbf{Keijzer5}: $\frac{3 x z}{(x - 10) y^{2}};$ $x,y \in E[-1,1,4]; z \in E[1,2,4]$
\item \textbf{Nguyen10}: $2 \sin(x) \cos(y);$ $x,y \in E[0,1,5]$
\item \textbf{Nguyen12}: $x^{4} - x^{3} + \frac{y^{2}}{2} - y;$ $x,y \in E[0,1,5]$
\item \textbf{Nguyen3}: $x^{5} + x^{4} + x^{3} + x^{2} + x;$ $x \in E[-1,1,20]$
\item \textbf{Nguyen4}: $x^{6} + x^{5} + x^{4} + x^{3} + x^{2} + x;$ $x \in E[-1,1,20]$
\item \textbf{Nguyen5}: $\sin(x^{2}) \cos(x) - 1;$ $x \in E[-1,1,20]$
\item \textbf{Nguyen6}: $\sin(x) + \sin(x + x^{2});$ $x \in E[-1,1,20]$
\item \textbf{Nguyen7}: $\ln(x + 1) + \ln(x^{2} + 1);$ $x \in E[0,2,20]$
\item \textbf{Nguyen9}: $\sin(x) + \sin(y^{2});$ $x,y \in E[0,1,5]$
\item \textbf{Sext}: $x^{6} - 2 x^{4} + x^{2};$ $x \in E[-1,1,20]$
\end{enumerate}

We use a standard implementation of GP from~\cite{} and chose parameters according to settings documented in~\cite{krawiecGECCO14}.
\textbf{Fixed Parameters}\label{appendix:fixed_parameters}

\begin{itemize}
\item \textbf{Tournament size}: 4
\item \textbf{Population size}: 100
\item \textbf{Number of Generations}: 250
\item \textbf{Maximum Program Tree Depth}: 17
\item \textbf{Function set}: $\{ +, -, *, /, \log, \exp, \sin, \cos, -x \}$
\item \textbf{Terminal set}: Only the features in the benchmark.
\item \textbf{Archive Capacity}: 50
\item \textbf{Mutation Rate $\mu$}: 0.1
\item \textbf{Crossover Rate with Archive configuration $\chi$}: 0.0
\item \textbf{Crossover Rate with GP $\chi$}: 0.9
\item \textbf{Archive-Based Crossover Rate $\alpha$}: 0.9
\item \textbf{REPTree}  defaults but no pruning
\item \textbf{\SCIKIT} defaults
\end{itemize}

First we replicated with 3 BGP algorithm configurations that use REPTree from KK et al's work on the symbolic regression benchmarks. These we name BGP2A, BGP4, BGP4A where the 2 indicates that no information from modeling flowed back to selection. The number 4 indicates that a program's fitness was modified by the model error $e$ and size $c$.   The suffix A indicates whether or not subprograms from the model were qualified for archive insertion and archive retrieval during BGP crossover. We observed results consistent with the prior work. Our open source software is available on GIT.

This allowed us to proceed to evaluate feature selection sensitivity based on modeling algorithm.

\subsection{Feature Selection Sensitivity}\label{sect:ftr-select}

For the algorithm configurations we adopt a clearer notation. We drop the BGP prefix and we use $M$ or $\hat M$ to designate whether model error was integrated into program fitness and $A$ of $\hat A$ to designate whether subprograms were were qualified for archive insertion and archive retrieval during BGP crossover.

Q1. Does the feature selection bias of the model step matter? 

\input{sections/decision_tree_ranks}

Look at Table~\ref{table:ranksReTreeVCART} for rankings by fitness, averaged across the benchmark set.


\input{sections/decision_tree_fitness}

Look at Table~\ref{table:fitnessReTreeVCART} for fitness, averaged across the benchmark set.


\begin{figure}[htbp]\begin{center}\includegraphics[width=0.99\linewidth]{sections/figures/figure_reptree.png}\caption{We take one run of Keijzer1 running with \REPTREE modeling and configuration BP4.   We plot on the first row  model error on the left and the fitness of the best program (right).  The plots on the second row show number of features of model  and number of subprograms in the best program (right). The plots on the third row show the ratio of number of model features to program subtrees (left) and ratio of model error to program fitness. Since the run is configured for  BP4 program fitness integrates both model error and complexity.}\label{fig:deepdive}\end{center}\end{figure}


Describe difference in implementations between Reptree and SKL-RepTree.\\
Need REPTREE and Scikit learn algorithm references and links to their code.

Compare REPTREE TO SICKIT LEARN for BGP 2A, 4a, 4 getting 6 combinations\\
Reference Table~\ref{table:avg_fitness} comparing average program error among BGP2A, BGP4, BGP4A, GP for 17 functions with REPTREE and ScikitLearn implementation.\\
Reference Table~\ref{table:avg_size} showing average program size among BGP2A, BGP4, BGP4A, GP for 17 functions with REPTREE and ScikitLearn implementation.\\
STDEV is in separate table in thesis, how to handle in paper?

Statistical testing required!!

Include a ranking table.  just on error of program and only 6 combinations (two choices of decision tree implementation) and 3 algorithms (2A,4,4A). 

Select 1 run, 1 dataset: what is \# of features in model of best individual at first and final generation (each generation)? What is fraction of \# of features in model to \# of subtrees in best individual at first and final generation (each generation)?  (Each generation) means we would have a plot (fitness on Y1 axis, features/fraction on Y2, Y3 and X is generations). We could amass these statistics for every dataset and 30 runs but lower priority than other tasks.

Segue to next subsection: Do features depend on one another? We're identifying their model value in the context of the same tree. They could be co-adapted and not very good with any other tree. We investigate further.

\subsection{Aggregate Trace Matrices}\label{sect:agg-features}

\input{sections/draws_average_ranks}
%\input{sections/fullpop_average_ranks}

And here is the average fitness table ....

\input{sections/average_fitness_DF}

\input{sections/ranks_all}


The question is what aggregations? What if we take everyone?

show discuss results of full-pop (call it 100\_XX)\\

Detailed study: take 1 run, 1 dataset (save seed!) and one algorithm of 2A,3,3A and one choice of decision tree.  What dataset? Choices: use hardest problem, or one that 2A,3,3A don't do as well as GP on. \\
How many programs have one or more features in the model?\\ how many programs have 2, 3, etc? This says something about co-adaptation. Were pairs of features subtrees where one was within the other?  \\what is the ratio of model features to total-features-in-pop? \\how many features are in the model (first, final generations?) (maybe plot every generation ( very low priority))

Now what if we discriminate the aggregation set by fitness so we're only identifying features from superior parts of the population?

We create 3 additional  aggregate trace matrices: 25, 50, 75,  where pop is sorted low to high by fitness (error + size) and then cut off at these different thresholds. 

Show/describe a longitudinal comparison of all 4 and add in best of program-trace resutls.

Does the discrimination by program fitness plus the aggregation among programs work better than just program trace? SHOW GRAND RANKING.

%\TODO{Insert my sketch of tables of results}
%
%\TODO{Not for Monday night but if we get accepted, we can increase the number of runs and robustness of the detailed results to encompass more datasets or more runs and provide average and standard deviation}
